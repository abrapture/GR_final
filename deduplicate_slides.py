#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è—Ö.

–†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "–∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö" —Å–ª–∞–π–¥–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–µ–¥—É—é—â–∏–π —Å–ª–∞–π–¥
–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–π –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–æ–π, –∞ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è.

–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏:
1. Sequential Diff - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–∞–π–¥–æ–≤
2. Content Hashing - –ø–æ–∏—Å–∫ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
3. Smart Merge - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–∞–π–¥–æ–≤
"""

import argparse
import sys
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
import hashlib
import difflib

try:
    from docling.document_converter import DocumentConverter, PdfFormatOption
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.document import DoclingDocument
    from docling_core.types.doc import TextItem, ImageRefMode
except ImportError:
    print("‚ùå –û—à–∏–±–∫–∞: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ Docling –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install docling")
    sys.exit(1)


class SlideDeduplicator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è—Ö.
    """
    
    def __init__(
        self,
        similarity_threshold: float = 0.85,
        min_text_length: int = 10,
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞.
        
        Args:
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–ª–∞–π–¥–æ–≤ (0.0-1.0)
            min_text_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É—á–µ—Ç–∞
        """
        self.similarity_threshold = similarity_threshold
        self.min_text_length = min_text_length
        self.stats = {
            'original_items': 0,
            'deduplicated_items': 0,
            'removed_duplicates': 0,
        }
    
    def extract_slides_content(self, doc: DoclingDocument) -> List[Dict]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ —Å–ª–∞–π–¥–∞–º (—Å—Ç—Ä–∞–Ω–∏—Ü–∞–º).
        
        Args:
            doc: DoclingDocument –æ–±—ä–µ–∫—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∫–∞–∂–¥–æ–≥–æ —Å–ª–∞–π–¥–∞
        """
        slides = []
        
        if not hasattr(doc, 'texts') or not doc.texts:
            return slides
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
        slides_by_page = defaultdict(list)
        
        for text_item in doc.texts:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ –ø—Ä–æ–≤enance
            page_num = 0
            if hasattr(text_item, 'prov') and text_item.prov:
                for prov in text_item.prov:
                    if hasattr(prov, 'page_no'):
                        page_num = prov.page_no
                        break
            
            if hasattr(text_item, 'text') and len(text_item.text.strip()) >= self.min_text_length:
                slides_by_page[page_num].append({
                    'text': text_item.text.strip(),
                    'label': str(text_item.label) if hasattr(text_item, 'label') else 'text',
                    'original_item': text_item,
                })
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
        for page_num in sorted(slides_by_page.keys()):
            slides.append({
                'page_num': page_num,
                'items': slides_by_page[page_num],
                'text_hash': self._hash_slide_content(slides_by_page[page_num]),
            })
        
        return slides
    
    def _hash_slide_content(self, items: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–ª–∞–π–¥–∞."""
        content = '\n'.join([item['text'] for item in items])
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _hash_text(self, text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à —Ç–µ–∫—Å—Ç–∞."""
        return hashlib.md5(text.strip().encode('utf-8')).hexdigest()
    
    def calculate_similarity(self, slide1: Dict, slide2: Dict) -> float:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ª–∞–π–¥–∞–º–∏.
        
        Args:
            slide1, slide2: –°–ª–æ–≤–∞—Ä–∏ —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º —Å–ª–∞–π–¥–æ–≤
            
        Returns:
            –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ (0.0-1.0)
        """
        texts1 = [item['text'] for item in slide1['items']]
        texts2 = [item['text'] for item in slide2['items']]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        matcher = difflib.SequenceMatcher(None, texts1, texts2)
        return matcher.ratio()
    
    def find_new_content(
        self,
        prev_slide: Dict,
        curr_slide: Dict,
    ) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –≤ —Ç–µ–∫—É—â–µ–º —Å–ª–∞–π–¥–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º.
        
        Args:
            prev_slide: –ü—Ä–µ–¥—ã–¥—É—â–∏–π —Å–ª–∞–π–¥
            curr_slide: –¢–µ–∫—É—â–∏–π —Å–ª–∞–π–¥
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        """
        prev_hashes = {self._hash_text(item['text']) for item in prev_slide['items']}
        
        new_items = []
        for item in curr_slide['items']:
            item_hash = self._hash_text(item['text'])
            if item_hash not in prev_hashes:
                new_items.append(item)
        
        return new_items
    
    def deduplicate_sequential(self, slides: List[Dict]) -> List[Dict]:
        """
        –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Å–ª–∞–π–¥ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.
        
        Args:
            slides: –°–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
            
        Returns:
            –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
        """
        if not slides:
            return []
        
        deduplicated = []
        
        # –ü–µ—Ä–≤—ã–π —Å–ª–∞–π–¥ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
        deduplicated.append(slides[0])
        
        for i in range(1, len(slides)):
            prev_slide = slides[i - 1]
            curr_slide = slides[i]
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity = self.calculate_similarity(prev_slide, curr_slide)
            
            if similarity >= self.similarity_threshold:
                # –°–ª–∞–π–¥—ã –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏ - –Ω–∞—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                new_items = self.find_new_content(prev_slide, curr_slide)
                
                if new_items:
                    # –°–æ–∑–¥–∞–µ–º —Å–ª–∞–π–¥ —Ç–æ–ª—å–∫–æ —Å –Ω–æ–≤—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
                    deduplicated.append({
                        'page_num': curr_slide['page_num'],
                        'items': new_items,
                        'text_hash': self._hash_slide_content(new_items),
                        'is_incremental': True,
                        'similarity_to_prev': similarity,
                    })
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∞–π–¥
            else:
                # –°–ª–∞–π–¥—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ - –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é
                deduplicated.append(curr_slide)
        
        return deduplicated
    
    def deduplicate_by_hashing(self, slides: List[Dict]) -> List[Dict]:
        """
        –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ —Ö–µ—à–∞–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ.
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –Ω–∞—Ö–æ–¥–∏–º —Å–ª–∞–π–¥—ã —Å –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã.
        
        Args:
            slides: –°–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
            
        Returns:
            –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
        """
        seen_hashes = set()
        deduplicated = []
        
        for slide in slides:
            slide_hash = slide['text_hash']
            
            if slide_hash not in seen_hashes:
                deduplicated.append(slide)
                seen_hashes.add(slide_hash)
            else:
                # –°–ª–∞–π–¥ —Å —Ç–∞–∫–∏–º —Ö–µ—à–µ–º —É–∂–µ –±—ã–ª - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                pass
        
        return deduplicated
    
    def smart_merge_slides(self, slides: List[Dict]) -> Dict:
        """
        –£–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–∞–π–¥–æ–≤ –≤ —Å–º—ã—Å–ª–æ–≤—ã–µ –±–ª–æ–∫–∏.
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–∞–π–¥—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Ö,
        —Å–æ—Ö—Ä–∞–Ω—è—è —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç.
        
        Args:
            slides: –°–ø–∏—Å–æ–∫ —Å–ª–∞–π–¥–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        """
        if not slides:
            return {'sections': []}
        
        sections = []
        current_section = {
            'start_page': slides[0]['page_num'],
            'end_page': slides[0]['page_num'],
            'items': slides[0]['items'].copy(),
            'item_hashes': {self._hash_text(item['text']) for item in slides[0]['items']},
        }
        
        for i in range(1, len(slides)):
            curr_slide = slides[i]
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–µ–π
            similarity = self.calculate_similarity(
                {'items': current_section['items']},
                curr_slide
            )
            
            if similarity >= self.similarity_threshold:
                # –°–ª–∞–π–¥ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ —Ç–µ–∫—É—â–µ–π —Å–µ–∫—Ü–∏–∏ - –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                for item in curr_slide['items']:
                    item_hash = self._hash_text(item['text'])
                    if item_hash not in current_section['item_hashes']:
                        current_section['items'].append(item)
                        current_section['item_hashes'].add(item_hash)
                
                current_section['end_page'] = curr_slide['page_num']
            else:
                # –ù–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è —Å–µ–∫—Ü–∏—è
                sections.append(current_section)
                current_section = {
                    'start_page': curr_slide['page_num'],
                    'end_page': curr_slide['page_num'],
                    'items': curr_slide['items'].copy(),
                    'item_hashes': {self._hash_text(item['text']) for item in curr_slide['items']},
                }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        sections.append(current_section)
        
        return {'sections': sections}
    
    def export_to_markdown(
        self,
        deduplicated_data: any,
        strategy: str,
        original_doc: DoclingDocument,
    ) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ Markdown.
        
        Args:
            deduplicated_data: –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            strategy: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
            original_doc: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π DoclingDocument
            
        Returns:
            Markdown –∫–æ–Ω—Ç–µ–Ω—Ç
        """
        md_lines = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        md_lines.append(f"# {original_doc.name if hasattr(original_doc, 'name') else '–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è'}")
        md_lines.append("")
        md_lines.append(f"*–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –º–µ—Ç–æ–¥–æ–º: {strategy}*")
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
        
        if strategy == 'smart_merge' and isinstance(deduplicated_data, dict):
            # –≠–∫—Å–ø–æ—Ä—Ç –ø–æ —Å–µ–∫—Ü–∏—è–º
            for idx, section in enumerate(deduplicated_data['sections'], 1):
                start = section['start_page'] + 1  # +1 –¥–ª—è —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–∏
                end = section['end_page'] + 1
                
                if start == end:
                    md_lines.append(f"## –†–∞–∑–¥–µ–ª {idx} (—Å–ª–∞–π–¥ {start})")
                else:
                    md_lines.append(f"## –†–∞–∑–¥–µ–ª {idx} (—Å–ª–∞–π–¥—ã {start}-{end})")
                
                md_lines.append("")
                
                for item in section['items']:
                    label = item['label']
                    text = item['text']
                    
                    if 'heading' in label.lower():
                        md_lines.append(f"### {text}")
                    elif 'list' in label.lower():
                        md_lines.append(f"- {text}")
                    else:
                        md_lines.append(f"{text}")
                    
                    md_lines.append("")
                
                md_lines.append("---")
                md_lines.append("")
        
        elif isinstance(deduplicated_data, list):
            # –≠–∫—Å–ø–æ—Ä—Ç —Å–ø–∏—Å–∫–æ–º —Å–ª–∞–π–¥–æ–≤
            for slide in deduplicated_data:
                page_num = slide['page_num'] + 1
                is_incremental = slide.get('is_incremental', False)
                
                if is_incremental:
                    md_lines.append(f"### ‚ûï –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ —Å–ª–∞–π–¥—É {page_num}")
                else:
                    md_lines.append(f"## –°–ª–∞–π–¥ {page_num}")
                
                if 'similarity_to_prev' in slide:
                    sim = slide['similarity_to_prev'] * 100
                    md_lines.append(f"*–°—Ö–æ–¥—Å—Ç–≤–æ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º: {sim:.1f}%*")
                
                md_lines.append("")
                
                for item in slide['items']:
                    label = item['label']
                    text = item['text']
                    
                    if 'heading' in label.lower():
                        md_lines.append(f"### {text}")
                    elif 'list' in label.lower():
                        md_lines.append(f"- {text}")
                    else:
                        md_lines.append(f"{text}")
                    
                    md_lines.append("")
                
                md_lines.append("---")
                md_lines.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        md_lines.append("## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏")
        md_lines.append("")
        md_lines.append(f"- –ò—Å—Ö–æ–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {self.stats['original_items']}")
        md_lines.append(f"- –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {self.stats['deduplicated_items']}")
        md_lines.append(f"- –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['removed_duplicates']}")
        
        if self.stats['original_items'] > 0:
            reduction = (self.stats['removed_duplicates'] / self.stats['original_items']) * 100
            md_lines.append(f"- –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—ä–µ–º–∞: {reduction:.1f}%")
        
        return '\n'.join(md_lines)
    
    def process_presentation(
        self,
        pdf_path: Path,
        strategy: str = 'smart_merge',
    ) -> Tuple[any, DoclingDocument]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏ —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π.
        
        Args:
            pdf_path: –ü—É—Ç—å –∫ PDF –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
                - 'sequential': –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                - 'hashing': –ø–æ —Ö–µ—à–∞–º
                - 'smart_merge': —É–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (–¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç)
        """
        print(f"\nüìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path.name}")
        print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        pipeline_options = PdfPipelineOptions()
        converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        
        result = converter.convert(str(pdf_path))
        doc = result.document
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–æ —Å–ª–∞–π–¥–∞–º
        slides = self.extract_slides_content(doc)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–ª–∞–π–¥–æ–≤: {len(slides)}")
        
        # –ü–æ–¥—Å—á–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        self.stats['original_items'] = sum(len(slide['items']) for slide in slides)
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        if strategy == 'sequential':
            deduplicated = self.deduplicate_sequential(slides)
        elif strategy == 'hashing':
            deduplicated = self.deduplicate_by_hashing(slides)
        elif strategy == 'smart_merge':
            deduplicated = self.smart_merge_slides(slides)
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
        
        # –ü–æ–¥—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if isinstance(deduplicated, dict):
            self.stats['deduplicated_items'] = sum(
                len(section['items']) for section in deduplicated['sections']
            )
        else:
            self.stats['deduplicated_items'] = sum(
                len(slide['items']) for slide in deduplicated
            )
        
        self.stats['removed_duplicates'] = (
            self.stats['original_items'] - self.stats['deduplicated_items']
        )
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {self.stats['original_items']}")
        print(f"   –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {self.stats['deduplicated_items']}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['removed_duplicates']}")
        
        if self.stats['original_items'] > 0:
            reduction = (self.stats['removed_duplicates'] / self.stats['original_items']) * 100
            print(f"   –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ: {reduction:.1f}%")
        
        return deduplicated, doc


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description='–î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è—é—â–µ–≥–æ—Å—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≤ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è—Ö',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

  # –£–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–ª–∞–π–¥–æ–≤)
  python deduplicate_slides.py presentation.pdf -o output.md

  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
  python deduplicate_slides.py presentation.pdf -o output.md -s sequential

  # –° –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π –ø–æ—Ä–æ–≥–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
  python deduplicate_slides.py presentation.pdf -o output.md -t 0.90

–°—Ç—Ä–∞—Ç–µ–≥–∏–∏:
  - smart_merge (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é): –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–ª–∞–π–¥—ã –≤ —Å–µ–∫—Ü–∏–∏
  - sequential: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Å–æ—Å–µ–¥–Ω–∏–µ —Å–ª–∞–π–¥—ã, –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–æ–≤–æ–µ
  - hashing: —É–¥–∞–ª—è–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —Å–ª–∞–π–¥—ã
        """
    )
    
    parser.add_argument(
        'input_file',
        type=str,
        help='–ü—É—Ç—å –∫ PDF –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        required=True,
        help='–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'
    )
    
    parser.add_argument(
        '-s', '--strategy',
        type=str,
        choices=['sequential', 'hashing', 'smart_merge'],
        default='smart_merge',
        help='–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: smart_merge)'
    )
    
    parser.add_argument(
        '-t', '--threshold',
        type=float,
        default=0.85,
        help='–ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–ª–∞–π–¥–æ–≤ (0.0-1.0, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.85)'
    )
    
    parser.add_argument(
        '--min-length',
        type=int,
        default=10,
        help='–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É—á–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)'
    )
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    input_path = Path(args.input_file)
    if not input_path.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_path}")
        sys.exit(1)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞
    print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä–∞")
    print("=" * 60)
    
    deduplicator = SlideDeduplicator(
        similarity_threshold=args.threshold,
        min_text_length=args.min_length,
    )
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    print(f"\nüìö –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏...")
    print("=" * 60)
    
    try:
        deduplicated_data, original_doc = deduplicator.process_presentation(
            input_path,
            strategy=args.strategy,
        )
        
        # –≠–∫—Å–ø–æ—Ä—Ç
        print(f"\nüíæ –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞...")
        print("=" * 60)
        
        markdown_content = deduplicator.export_to_markdown(
            deduplicated_data,
            args.strategy,
            original_doc,
        )
        
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(markdown_content, encoding='utf-8')
        
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n" + "=" * 60)
        print("–ò–¢–û–ì–ò")
        print("=" * 60)
        print(f"üìä –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {deduplicator.stats['original_items']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        print(f"üìä –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {deduplicator.stats['deduplicated_items']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        print(f"üìä –≠–∫–æ–Ω–æ–º–∏—è: {deduplicator.stats['removed_duplicates']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        if deduplicator.stats['original_items'] > 0:
            reduction = (deduplicator.stats['removed_duplicates'] / 
                        deduplicator.stats['original_items']) * 100
            print(f"üìä –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –æ–±—ä–µ–º–∞: {reduction:.1f}%")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

